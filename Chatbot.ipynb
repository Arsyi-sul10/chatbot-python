{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-PzrwtaIIO1"
      },
      "source": [
        "Hello , Welcome to My Project\r\n",
        "My Name Is Muhammad Arsyi Sulthoni , This is my first project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqDKB9aNHUs0"
      },
      "source": [
        "## NLP\n",
        "NLP adalah cara komputer untuk menganalisis, memahami, dan mendapatkan makna dari bahasa manusia dengan cara yang cerdas dan berguna. Dengan memanfaatkan NLP, pengembang dapat mengatur dan menyusun pengetahuan untuk melakukan tugas-tugas seperti peringkasan otomatis, terjemahan, pengenalan entitas, ekstraksi hubungan, analisis sentimen, pengenalan ucapan, dan segmentasi topik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbS9hVn0HUst"
      },
      "source": [
        "# ChatBot dengan NLTK\n",
        "\n",
        "![Alt text](https://cdn-images-1.medium.com/max/800/1*pPcVfZ7i-gLMabUol3zezA.gif)\n",
        "\n",
        "Sejarah chatbots dimulai pada tahun 1966 ketika program komputer yang disebut ELIZA ditemukan oleh Weizenbaum. Itu meniru bahasa psikoterapis hanya dari 200 baris kode. Kalian masih bisa mencobanya dengannya di sini: [Eliza](http://psych.fullerton.edu/mbirnbaum/psych101/Eliza.htm?utm_source=ubisend.com&utm_medium=blog-link&utm_campaign=ubisend). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvZgPbZ6HUs1"
      },
      "source": [
        "## Import Library nya"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6cNseqTHUs1"
      },
      "source": [
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlN00lsMI5cF"
      },
      "source": [
        "# Fungsi Librarynya :\r\n",
        "io : memungkinkan kita untuk mengelola operasi input dan output sebuah file.\r\n",
        "\r\n",
        "random : yaitu mengimpor modul secara acak, yang berisi berbagai hal yang berkaitan dengan pembuatan nomor acak.(random number)\r\n",
        "\r\n",
        "string : untuk memproses strings pada python\r\n",
        "\r\n",
        "warnings : cara untuk memperingatkan programmer tentang perubahan language atau fitur dari library untuk mengantisipasi perubahan yang tidak kompatibel dengan Python 3.0\r\n",
        "\r\n",
        "numpy : library yang berguna untuk array, biasa digunakan untuk , linear aljabar,matriks dll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRobtWFHHUs2"
      },
      "source": [
        "## Downloading dan penginstallan NLTK\n",
        "NLTK (Natural Language Toolkit) adalah platform yang digunakan untuk membuat program Python agar dapat bekerja dengan data bahasa manusia(natural). Ini menyediakan interface yang mudah digunakan ke lebih dari 50 sumber korpora dan leksikal seperti WordNet, bersama dengan rangkaian text processing libraries untuk klasifikasi, tokenisasi, stemming, tagging, parsing, dan semantic reasoning, wrappers untuk library NLP strength industry.\n",
        "\n",
        "[Natural Language Processing with Python](http://www.nltk.org/book/)\n",
        "\n",
        "Spesifikasi platformnya [here](https://www.nltk.org/install.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLzZW57WHUs2",
        "outputId": "ae4bf81c-92d6-41c6-e917-7fffab422086"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5B-tMoQHUs4"
      },
      "source": [
        "### Menginstall NLTK Packages\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkog6FSeHUs4",
        "outputId": "00802956-a765-4a4b-c6af-ae719bf30666"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True) # untuk download packages\n",
        "#nltk.download('punkt') # first-time use only\n",
        "#nltk.download('wordnet') # first-time use only"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1ePf4LEHUs4"
      },
      "source": [
        "## Reading in the corpus\n",
        "\n",
        "Sebagai contoh, saya akan menggunakan halaman Wikipedia untuk chatbots sebagai korpus saya. mengcopy konten dari halaman dan letakkan di file teks bernama 'chatbot.txt'. jadi saya akan memasukkan chatbox.txt dibawah ini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "BNj0P83_HUs5",
        "outputId": "6dc11e09-ef0f-4635-f161-d1e211922a39"
      },
      "source": [
        "f=open('chatbot.txt','r',errors = 'ignore')\n",
        "raw=f.read()\n",
        "raw = raw.lower()# converts to lowercase"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7efe070b5371>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'chatbot.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# converts to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'chatbot.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrVS3JsrHUs5"
      },
      "source": [
        "\n",
        "Masalah utama dengan data teks adalah semuanya dalam format teks (string). Namun, algoritme pembelajaran mesin memerlukan semacam vektor fitur numerik untuk melakukan tugas tersebut. Jadi sebelum kita mulai dengan proyek NLP apa pun, kita perlu memprosesnya terlebih dahulu agar ideal untuk bekerja. Pra-pemrosesan teks dasar meliputi:\n",
        "\n",
        "Mengonversi seluruh teks menjadi huruf besar atau kecil, sehingga algoritme tidak memperlakukan kata yang sama dalam kasus berbeda sebagai berbeda\n",
        "\n",
        "Tokenisasi: Tokenisasi hanyalah istilah yang digunakan untuk menggambarkan proses mengubah string teks normal menjadi daftar token yaitu kata-kata yang sebenarnya kita inginkan. Kalimat tokenizer dapat digunakan untuk mencari daftar kalimat dan Word tokenizer dapat digunakan untuk mencari daftar kata dalam string.\n",
        "\n",
        "Paket data NLTK menyertakan tokenizer Punkt terlatih untuk bahasa Inggris.\n",
        "\n",
        "Menghapus Kebisingan yaitu segala sesuatu yang tidak ada dalam angka atau huruf standar.\n",
        "Menghapus kata-kata Berhenti. Kadang-kadang, beberapa kata yang sangat umum yang tampaknya tidak berguna dalam membantu memilih dokumen yang sesuai dengan kebutuhan pengguna dikecualikan dari kosakata sepenuhnya. Kata-kata ini disebut kata-kata berhenti\n",
        "Stemming: Stemming adalah proses mereduksi kata-kata yang mengalami infleksi (atau kadang-kadang diturunkan) menjadi bentuk stem, base atau root - umumnya dalam bentuk kata tertulis. Contoh jika kita membendung kata-kata berikut: “Stems”, “Stemming”, “Stemmed”, “and Stemtization”, hasilnya akan menjadi satu kata “Stem”.\n",
        "Lemmatisasi: Varian kecil dari stemming adalah lemmatisasi. Perbedaan utama antara keduanya adalah, bahwa, stemming seringkali dapat membuat kata-kata yang tidak ada, sedangkan lemma adalah kata-kata yang sebenarnya. Jadi, akar kata Anda, yang berarti kata yang Anda akhiri, bukanlah sesuatu yang bisa Anda cari di kamus, tetapi Anda bisa mencari lemma. Contoh Lemmatisasi adalah bahwa \"lari\" adalah bentuk dasar untuk kata-kata seperti \"lari\" atau \"lari\" atau kata \"lebih baik\" dan \"baik\" ada dalam lemma yang sama sehingga dianggap sama\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHeqIErHUs6"
      },
      "source": [
        "## Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH0EaUOwHUs6"
      },
      "source": [
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkg7SiLTHUs6"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "\n",
        "saya sekarang akan mendefinisikan fungsi yang disebut LemTokens yang akan mengambil sebagai masukan token dan mengembalikan token yang dinormalisasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMaxTw4yHUs7"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aSokzNQHUs7"
      },
      "source": [
        "## Keyword matching\n",
        "\n",
        "Selanjutnya, kita akan mendefinisikan fungsi untuk salam oleh bot, yaitu jika input pengguna adalah salam, bot akan mengembalikan respons salam. ELIZA menggunakan pencocokan kata kunci sederhana untuk salam. Kami akan menggunakan konsep yang sama di sini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOG5r4PlHUs7"
      },
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkczSzevHUs7"
      },
      "source": [
        "## Generating Response\n",
        "\n",
        "### Bag of Words\n",
        "Setelah tahap preprocessing awal, kita perlu mengubah teks menjadi vektor (atau larik) angka yang bermakna. Bag-of-words merupakan representasi dari teks yang menggambarkan kemunculan kata-kata dalam sebuah dokumen. Ini melibatkan dua hal:\n",
        "\n",
        "* Kosakata dari kata-kata yang dikenal.\n",
        "\n",
        "* Ukuran keberadaan kata-kata yang dikenal.\n",
        "\n",
        "Mengapa disebut “sekantong” kata-kata? Hal ini karena informasi tentang urutan atau struktur kata dalam dokumen akan dibuang dan modelnya hanya ** terkait dengan apakah kata yang diketahui muncul dalam dokumen, bukan di mana kata tersebut muncul dalam dokumen. **\n",
        "\n",
        "Intuisi di balik Bag of Words adalah bahwa dokumen itu serupa jika memiliki konten yang serupa. Selain itu, kita dapat mempelajari sesuatu tentang arti dokumen dari isinya saja.\n",
        "\n",
        "contoh jika dictionarynya {Learning, is, the, not, great}, dan kita akan selanjutnya melakukan vectorize “Learning is great”,  maka kita akan dapat vektor: (1, 1, 0, 0, 1).\n",
        "\n",
        "\n",
        "### TF-IDF Approach\n",
        "Masalah dengan pendekatan Bag of Words adalah bahwa kata-kata yang sangat sering mulai mendominasi dalam dokumen (misalnya skor yang lebih besar), tetapi mungkin tidak mengandung banyak \"konten informasi\". Selain itu, ini akan memberi bobot lebih pada dokumen yang lebih panjang daripada dokumen yang lebih pendek.\n",
        "\n",
        "Salah satu pendekatannya adalah mengubah skala frekuensi kata dengan seberapa sering muncul di semua dokumen sehingga skor untuk kata yang sering seperti \"the\" yang juga sering muncul di semua dokumen akan dikenai sanksi. Pendekatan penilaian ini disebut Term Frequency-Inverse Document Frequency, atau disingkat TF-IDF, di mana:\n",
        "\n",
        "** Term Frequency: adalah penilaian dari frekuensi kata dalam dokumen saat ini. **\n",
        "\n",
        "``\n",
        "TF = (Berapa kali term t muncul di dokumen) / (Jumlah term di dokumen)\n",
        "``\n",
        "\n",
        "** Inverse Document Frequency: adalah penilaian seberapa jarang kata tersebut ada di seluruh dokumen. **\n",
        "\n",
        "``\n",
        "IDF = 1 + log (N / n), dimana, N adalah banyaknya dokumen dan n adalah banyaknya dokumen dimana term t muncul.\n",
        "``\n",
        "### Kesamaan Cosine\n",
        "\n",
        "Bobot Tf-idf adalah bobot yang sering digunakan dalam pengambilan informasi dan penambangan teks. Bobot ini adalah ukuran statistik yang digunakan untuk mengevaluasi seberapa penting sebuah kata bagi dokumen dalam kumpulan atau korpus\n",
        "\n",
        "``\n",
        "Kesamaan Kosinus (d1, d2) = Hasil kali titik (d1, d2) / || d1 || * || d2 ||\n",
        "``\n",
        "dimana d1, d2 adalah dua vektor bukan nol.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKfM22swHUs8"
      },
      "source": [
        "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix01ALcLHUs9"
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]\n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqCfY8McHUs9"
      },
      "source": [
        "Terakhir, saya akan menambahkan kata yang saya ingin bot katakan saat memulai dan mengakhiri percakapan tergantung pada masukan user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZEdXBjsHUs9",
        "outputId": "bb4d90f5-d181-49ac-978b-88d362509a4e"
      },
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u1fKxDVHUs-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}